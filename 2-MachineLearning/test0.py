import os.path
import time

import torch.optim as optim
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.utils.data
import torch.utils.data.distributed
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.autograd import Variable
from torchvision.models import densenet121
import numpy as np




################# 修改数据集目录处 #################

samples_root = r"/root/autodl-tmp/3ChannelOrg"

################# 修改数据集目录处 #################




# 如果两个教程里面都用了动态的梯度变换，那么这一个步骤应该是有用的。
def adjust_learning_rate(optimizer, epoch):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    modellrnew = modellr * (0.1 ** (epoch // 50)) #向负取整商
    print("lr:", modellrnew)
    for param_group in optimizer.param_groups:
        param_group['lr'] = modellrnew


# 定义训练过程

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    sum_loss = 0 # 总损失

    # total_num = len(train_loader.dataset) # 样本总数
    # print(total_num, len(train_loader)) # 样本总数、batch总数

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data).to(device), Variable(target).to(device) #将数据在tensor角度、使用设备角度规范化
        output = model(data) #网络给出预测结果
        loss = criterion(output, target) #计算误差函数 ->Tensor target == 9
        optimizer.zero_grad() #梯度归0，避免梯度积累
        loss.backward() #反向传播，计算梯度
        optimizer.step() #参数更新
        print_loss = loss.data.item() #得到损失
        # train_batch_lost_arr.append(print_loss)
        sum_loss += print_loss # 一个batch的loss。一个batch的loss实际上就是batch里面的多个样本的loss之和。只不过这里使用了矩阵进行运算，所以看起来有点抽象。
        # if (batch_idx + 1) % 200 == 0:
        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        #                                                                 epoch,
        #                                                                 (batch_idx + 1) * len(data),
        #                                                                 len(train_loader.dataset),
        #                                                                 100. * (batch_idx + 1) / len(train_loader),
        #                                                                 loss.item()))

        # 存储可视化文件所需节点信息

    ave_loss = sum_loss / len(train_loader)
    # train_epoch_lost_arr.append(ave_loss)
    print('Train set| Average loss: {:.4f}'.format(ave_loss))


# 验证过程
def val(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    total_num = len(test_loader.dataset)
    # print(total_num, len(test_loader))
    with torch.no_grad():
        for data, target in test_loader:
            data, target = Variable(data).to(device), Variable(target).to(device)
            output = model(data)
            loss = criterion(output, target)
            _, pred = torch.max(output.data, 1)
            correct += torch.sum(pred == target)
            print_loss = loss.data.item()
            test_loss += print_loss
        correct = correct.data.item()
        acc = correct / total_num
        avgloss = test_loss / len(test_loader)

        # 取最大验证效果为最终验证效果
        global val_acc
        val_acc = acc if acc > val_acc else val_acc

        print('Val set  | Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            avgloss, correct, len(test_loader.dataset), 100 * acc))


# 训练
if __name__ == "__main__":

    time_begin = time.time()

    # 设置全局参数
    modellr = 0.01
    BATCH_SIZE = 32
    EPOCHS = 7
    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    # train_epoch_lost_arr = []

    # 每2000条样本记录一次loss，多个epoch的数据融合在一起。
    all_batch_lost_arr = []

    val_acc = 0



    # 数据预处理
    # transform = transforms.Compose([
    #     transforms.Resize((224, 224)),
    #     transforms.ToTensor(),
    #     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    #
    # ])
    # transform_test = transforms.Compose([
    #     transforms.Resize((224, 224)),
    #     transforms.ToTensor(),
    #     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    # ])

    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    transform_test = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])



    # 导入数据
    dataset_train = datasets.ImageFolder(os.path.join(samples_root,"train"), transform)
    dataset_test = datasets.ImageFolder(os.path.join(samples_root,"test"), transform_test)

    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True) #将Windows风格的数据集Python化（便于使用迭代器方法）
    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)



    # 实例化模型并且移动到GPU
    criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数
    model_ft = densenet121(pretrained=True)  # 调取已经选练好参数的模型
    num_ftrs = model_ft.classifier.in_features
    model_ft.classifier = nn.Linear(num_ftrs, 9)
    model_ft.to(DEVICE)
    # 选择简单暴力的Adam优化器，学习率调低
    optimizer = optim.Adam(model_ft.parameters(), lr=modellr)



    #训练 & 验证
    for epoch in range(1, EPOCHS + 1):
        print("Epoch {}".format(epoch))
        adjust_learning_rate(optimizer, epoch) #这一步需要调整到项目适配的程度
        train(model_ft, DEVICE, train_loader, optimizer, epoch)
        val(model_ft, DEVICE, test_loader)

    # torch.save(model_ft, 'model.pth')

    # 存储可视化文件
    view = np.asarray(all_batch_lost_arr)
    np.save("epoch_lost_arr.npy",view)

    view = np.asarray(all_batch_lost_arr)
    np.save("batch.lost_arr.npy", view)

    view = np.asarray(val_acc)
    np.save("val_acc.npy",view)

    time_end = time.time()
    print("Total taken time: {:0.1f} min\n".format((time_end - time_begin)/60.0))